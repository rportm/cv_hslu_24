{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Segmentation of Multiple Sclerosis with Brain MRI dataset"
      ],
      "metadata": {
        "id": "yjcgwoCAWQ-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Jupyter notebook is designed to explore and utilize a Brain MRI dataset of patients with Multiple Sclerosis (MS) for the purpose of predicting lesions. Multiple Sclerosis is a chronic illness characterized by the presence of lesions in the brain and spinal cord, leading to a wide range of neurological symptoms. Detecting and monitoring these lesions using MRI is a critical part of diagnosing and managing the disease.\n",
        "\n",
        "The dataset used in this notebook is sourced from a research publication by M Muslim (2022) and includes MRI scans along with consensus manual lesion segmentation. This provides an adequate resource for training and evaluating AI models in medical imaging applications.\n"
      ],
      "metadata": {
        "id": "3XfLHbelWjXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "94JIDVBZW1uE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will download the Brain MRI dataset from the Mendeley Data repository, organize the data into training, testing, and validation sets, and prepare it for analysis.\n",
        "\n",
        "The dataset consists of MRI scans from 60 patients, each stored in separate folders. We will:\n",
        "\n",
        "1. Download the dataset, which is provided as a ZIP file.\n",
        "2. Extract the contents of the ZIP file.\n",
        "3. Randomly allocate 40 patient folders to a training set, 10 to a testing set, and the remaining 10 to a validation set, ensuring reproducibility by setting a random seed.\n"
      ],
      "metadata": {
        "id": "LndoAI0yW304"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYqrQKBKVeXK",
        "outputId": "9dcae968-1662-4b8f-fdc1-215861a41a56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded Brain MRI Dataset of Multiple Sclerosis with Consensus Manual Lesion Segmentation and Patient Meta Information.zip\n",
            "Extracted Brain MRI Dataset of Multiple Sclerosis with Consensus Manual Lesion Segmentation and Patient Meta Information.zip to brain_mri_dataset\n",
            "Organized data into train, test, and validation folders.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "\n",
        "N_TRAIN = 40\n",
        "N_TEST = 10\n",
        "\n",
        "# Define the URL of the dataset\n",
        "dataset_url = \"https://data.mendeley.com/public-files/datasets/8bctsm8jz7/files/9356efeb-dcd8-4213-a2d4-8febe9f1a5db/file_downloaded\"\n",
        "zip_filename = \"Brain MRI Dataset of Multiple Sclerosis with Consensus Manual Lesion Segmentation and Patient Meta Information.zip\"\n",
        "extracted_folder = \"brain_mri_dataset\"\n",
        "\n",
        "# Define the root folder for data organization\n",
        "data_folder = \"data\"\n",
        "\n",
        "# Define destination directories under the data folder\n",
        "train_folder = os.path.join(data_folder, \"train\")\n",
        "test_folder = os.path.join(data_folder, \"test\")\n",
        "validation_folder = os.path.join(data_folder, \"validation\")\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Function to download the dataset\n",
        "def download_dataset(url, filename):\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(filename, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=1024):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "    print(f\"Downloaded {filename}\")\n",
        "\n",
        "# Function to extract the dataset\n",
        "def extract_dataset(zip_filename, extract_to):\n",
        "    with ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(f\"Extracted {zip_filename} to {extract_to}\")\n",
        "\n",
        "# Function to organize patient data into train, test, and validation folders\n",
        "def organize_data(extracted_folder, train_folder, test_folder, validation_folder):\n",
        "    # Create the data folder if it doesn't exist\n",
        "    os.makedirs(data_folder, exist_ok=True)\n",
        "\n",
        "    # Get the list of patient folders\n",
        "    patient_folders = [folder for folder in os.listdir(extracted_folder) if folder.startswith(\"Patient\")]\n",
        "\n",
        "    # Shuffle the list of patient folders\n",
        "    random.shuffle(patient_folders)\n",
        "\n",
        "    # Split into train, test, and validation\n",
        "    train_patients = patient_folders[:N_TRAIN]\n",
        "    test_patients = patient_folders[N_TRAIN:N_TRAIN+N_TEST]\n",
        "    validation_patients = patient_folders[N_TRAIN+N_TEST:]\n",
        "\n",
        "    # Create directories if they don't exist\n",
        "    os.makedirs(train_folder, exist_ok=True)\n",
        "    os.makedirs(test_folder, exist_ok=True)\n",
        "    os.makedirs(validation_folder, exist_ok=True)\n",
        "\n",
        "    # Move patient folders to respective directories\n",
        "    for patient in train_patients:\n",
        "        shutil.move(os.path.join(extracted_folder, patient), train_folder)\n",
        "\n",
        "    for patient in test_patients:\n",
        "        shutil.move(os.path.join(extracted_folder, patient), test_folder)\n",
        "\n",
        "    for patient in validation_patients:\n",
        "        shutil.move(os.path.join(extracted_folder, patient), validation_folder)\n",
        "\n",
        "    print(\"Organized data into train, test, and validation folders.\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Download the dataset\n",
        "    download_dataset(dataset_url, zip_filename)\n",
        "\n",
        "    # Extract the dataset\n",
        "    extract_dataset(zip_filename, extracted_folder)\n",
        "\n",
        "    # Organize the data into train, test, and validation folders\n",
        "    organize_data(extracted_folder, train_folder, test_folder, validation_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking resolution"
      ],
      "metadata": {
        "id": "XefqpngDy_Gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nibabel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btEMF2Woz4DC",
        "outputId": "9d686385-c87c-478e-b6fb-6514d28edf22"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (5.0.1)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from nibabel) (1.26.4)\n",
            "Requirement already satisfied: packaging>=17 in /usr/local/lib/python3.10/dist-packages (from nibabel) (24.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel) (71.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nibabel as nib\n",
        "import pandas as pd\n",
        "\n",
        "# Define the root directory containing the train, test, and validation subfolders\n",
        "data_dir = \"data\"  # Change this to the location of your \"data\" folder\n",
        "\n",
        "# List to store resolution information\n",
        "resolution_info = []\n",
        "\n",
        "# Iterate through each subfolder (train, test, validation)\n",
        "for subset in [\"train\", \"test\", \"validation\"]:\n",
        "    subset_path = os.path.join(data_dir, subset)\n",
        "\n",
        "    # Check if the subset directory exists\n",
        "    if os.path.exists(subset_path):\n",
        "\n",
        "        # Iterate through each patient folder in the current subset\n",
        "        for patient_folder in os.listdir(subset_path):\n",
        "            patient_path = os.path.join(subset_path, patient_folder)\n",
        "\n",
        "            # Make sure it's a directory\n",
        "            if os.path.isdir(patient_path):\n",
        "\n",
        "                # Iterate through each file in the patient folder\n",
        "                for file in os.listdir(patient_path):\n",
        "                    if file.endswith(\".nii\"):\n",
        "                        file_path = os.path.join(patient_path, file)\n",
        "\n",
        "                        # Load the .nii file using nibabel\n",
        "                        img = nib.load(file_path)\n",
        "                        header = img.header\n",
        "\n",
        "                        # Get the voxel dimensions (resolution)\n",
        "                        voxel_dimensions = header.get_zooms()  # Returns (x, y, z) dimensions\n",
        "\n",
        "                        # Get the shape of the image data\n",
        "                        image_shape = img.shape\n",
        "\n",
        "                        # Store the information in the list\n",
        "                        resolution_info.append({\n",
        "                            \"Subset\": subset,\n",
        "                            \"Patient\": patient_folder,\n",
        "                            \"File\": file,\n",
        "                            \"Voxel Dimensions\": voxel_dimensions,\n",
        "                            \"Image Shape\": image_shape\n",
        "                        })\n",
        "\n",
        "# Create a DataFrame to display the resolution information\n",
        "resolution_df = pd.DataFrame(resolution_info)\n",
        "\n",
        "# Display the DataFrame to check resolutions\n",
        "print(resolution_df)\n",
        "\n",
        "# Optional: Save the DataFrame to a CSV file for further analysis\n",
        "resolution_df.to_csv(\"resolution_overview.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8kPq2O8y-sI",
        "outputId": "a818f819-e46f-4372-b955-efeb4723d0f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Subset     Patient                   File Voxel Dimensions  \\\n",
            "0         train   Patient-1               1-T1.nii  (1.0, 1.0, 1.0)   \n",
            "1         train   Patient-1  1-LesionSeg-Flair.nii  (1.0, 1.0, 1.0)   \n",
            "2         train   Patient-1               1-T2.nii  (1.0, 1.0, 1.0)   \n",
            "3         train   Patient-1     1-LesionSeg-T1.nii  (1.0, 1.0, 1.0)   \n",
            "4         train   Patient-1            1-Flair.nii  (1.0, 1.0, 1.0)   \n",
            "..          ...         ...                    ...              ...   \n",
            "355  validation  Patient-17    17-LesionSeg-T2.nii  (1.0, 1.0, 1.0)   \n",
            "356  validation  Patient-17              17-T2.nii  (1.0, 1.0, 1.0)   \n",
            "357  validation  Patient-17              17-T1.nii  (1.0, 1.0, 1.0)   \n",
            "358  validation  Patient-17           17-Flair.nii  (1.0, 1.0, 1.0)   \n",
            "359  validation  Patient-17    17-LesionSeg-T1.nii  (1.0, 1.0, 1.0)   \n",
            "\n",
            "        Image Shape  \n",
            "0    (512, 512, 19)  \n",
            "1    (256, 256, 23)  \n",
            "2    (256, 256, 19)  \n",
            "3    (512, 512, 19)  \n",
            "4    (256, 256, 23)  \n",
            "..              ...  \n",
            "355  (256, 256, 20)  \n",
            "356  (256, 256, 20)  \n",
            "357  (512, 464, 18)  \n",
            "358  (256, 256, 18)  \n",
            "359  (512, 464, 18)  \n",
            "\n",
            "[360 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References"
      ],
      "metadata": {
        "id": "0RCgVviIVm9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "M Muslim, Ali (2022), “Brain MRI Dataset of Multiple Sclerosis with Consensus Manual Lesion Segmentation and Patient Meta Information”, Mendeley Data, V1, doi: 10.17632/8bctsm8jz7.1"
      ],
      "metadata": {
        "id": "saYIlVIOVmA8"
      }
    }
  ]
}